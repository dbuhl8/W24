\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{xcolor}

\title{Homework 2: Report}
\author{Dante Buhl}
\date{Jan $20^{th}$ 2024}

\begin{document}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bmp}[1]{\begin{minipage}{#1\textwidth}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\K}{\bs{\mathrm{K}}}
\newcommand{\m}{\bs{\mu}_*}
\newcommand{\s}{\bs{\Sigma}_*}
\newcommand{\dt}{\Delta t}
\newcommand{\tr}[1]{\text{Tr}(#1)}

\maketitle


\begin{enumerate}

\item
Let $A \in C^{m\times m}$ be both upper-triangular and unitary. Show that A is a
diagonal matrix. Does the same hold if $A \in C^{m\times m}$ is both lower-triangular
and unitary?

\begin{proof}

\textbf{ (Upper Triangular, by Induction)}

Assume matrix $A \in \C^{m \times m}$ is unitary and is upper triangular such that,

\[ 
    A^*A = \mathrm{I}_m = AA^*
\]

\[
    A = \left[\begin{array}{c c c c}
        a_{11} & a_{12} & \cdots & a_{1m} \\
        0 & a_{22} & \cdots & a_{2m} \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots  & a_{mm}
        \end{array}\right]
\] 

Where $A^*$ is the complex transpose matrix of $A$. We have then that $A^*$ is of the form, 

\[
    A^* = \left[\begin{array}{c c c c}
        \overline{a_{11}} & 0 & \cdots & 0 \\
        \overline{a_{12}} & \overline{a_{22}} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        \overline{a_{1m}} & \overline{a_{2m}} & \cdots  & \overline{a_{mm}}
        \end{array}\right]
\] 

Then the product of matrix multiplication of $A^*$ and $A$ is then defined as $B$, (i.e. $B = A^*A$), and because A is a Unitary matrix, is equal to $\mathrm{I}$. 

\textbf{(Base Case)}: 

Now assume that the elements above the diagonal in $A$ are non-zerp. Next examine the $(1, 1)$ and $(2, 1)$ cells of the matrix product, $B$. By the operation of Matrix multiplication we should have, 
\[
    B(2, 1) = a_{11} \cdot \overline{a_{12}} = \mathrm{I}_m(2, 1) = 0
\]
\[
    B(1, 1) = a_{11} \cdot \overline{a_{11}} = \mathrm{I}_m(1, 1) = 1
\]
From this, we know that $a_{11} \neq 0$ and $\overline{a_{11}} \neq 0$. But we have that the product of $a_{11} \cdot \overline{a_{12}} = 0$. Since we have that  $a_{11} \neq 0$, we must therefore have that $\overline{a_{12}} = 0$ and by the definition of a complex conjugate, $a_{12} = 0$. 

\textbf{(Inductive Step)}

We need to show that for a integer $k \le m-1$ all of the columns of matrix $A$, $\vec{C}_i$, up to $\vec{C}_k$ is of the form, 
\[
    \vec{C}_i = \left[\begin{array}{c}
                0 \\
                \vdots \\
                a_{ii} \\
                \vdots \\
                0
                \end{array}\right]
\]
then $\vec{C}_{k+1}$ is also of the same form. We have that in the matrix product between $A^*$ and $A$, $B$, then the i-th row of $B$ is defined as the inner produt between the i-th row of $A^*$, $\vec{r}_i^*$ and the j-th column of $A$, $\vec{c}_j$.  
\[
    B(i, :) = [(\vec{r}_i^*, \vec{c}_1), (\vec{r}_2^*, \vec{c}_2), \cdots, (\vec{r}_i^*, \vec{c}_j)] = \mathrm{I}_m(i, :) = [0, \cdots, 1, \cdots, 0]
\]

Look at the $(k+1)$-th row of B. We have from the given form of the columns, $\{\vec{c}_1, \cdots, \vec{c}_k\}$, 
\[
    (\vec{r}_{k+1}^*, \vec{c}_j) = \overline{a_{j(k+1)}}\cdot a_{jj} = \left\{\begin{array}{c c c}
                                                                0 & \text{if, } & j \neq k+1\\
                                                                1 & \text{if, } & j = k+1
                                                            \end{array}\right\}, i, j < k+1
\]
We also have that each $a_{ii} \neq 0$. Thereby, for all $j < k+1$, 
\[\overline{a_{j(k+1)}} = 0 \implies a_{j(k+1)} = 0\]We now write the column, $\vec{c}_{k+1}$. 
\[  
    \vec{c}_{k+1} = \left[\begin{array}{c}
                    0 \\
                    \vdots \\
                    a_{(k+1)(k+1)} \\
                    \vdots \\
                    0
                    \end{array}\right]
\]
Therefore, we have that $\vec{c}_{k+1}$ is of the same form as $\vec{c}_{i}, i \le k$. By induction, each column of $A$ is of this form. Therefore, $A$ is a diagonal matrix!


\end{proof}

\begin{proof}

\textbf{(Lower Triangular, by case of Upper Triangular)}

Assume as before,  $A \in \C^{m \times m}$ is unitary and is lower triangular such that,

\[ 
    A^*A = \mathrm{I}_m = AA^*
\]

\[
    A = \left[\begin{array}{c c c c}
        a_{11} & 0 & \cdots & 0 \\
        a_{21} & a_{22} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots  & a_{mm}
        \end{array}\right]
\] 

Where $A^*$ is the complex transpose matrix of $A$. We have then that $A^*$ is of the form, 

\[
    A^* = \left[\begin{array}{c c c c}
        \overline{a_{11}} & \overline{a_{21}} & \cdots & \overline{a_{m1}} \\
        0 & \overline{a_{22}} & \cdots & \overline{a_{m2}} \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots  & \overline{a_{mm}}
        \end{array}\right]
\] 

We then define the matrix $C = A^*$, $C^* = A$. Notice that C is an upper triangular, unitary matrix. By the previous proof, $C$ is a diagonal matrix. Notice all of its ``off-diagonal'' elements are zero. As a consequnce, all (i, j)-elements of $C$ which are zero imply that (j, i)-elements of $C^*$ are zero. Therefore, $C^* = A$ is a diagonal matrix.


\end{proof}

\item 
Prove the following in each problem.
\begin{enumerate}

    \item
    Let $A \in \C^{m \times m}$ be invertible and $\lambda \neq 0$ is an eigenvalue of $A$. Showthat $\lambda^{-1}$ is an eigenvalue of $A^{-1}$.

    \begin{proof}

    Take any $A \in \C^{m\times m}$ to be invertible. Then we have inverse, $A^{-1}$ exists such that, 
    \[
        AA^{-1} = \mathrm{I}_m = A^{-1}A
    \]
    We also have by the fact that $\lambda \neq 0$ is an eigenvalue of $A$ that, 
    \[
        \text{det}(A - \lambda\mathrm{I}_m) = 0
    \]
    We can substitute for $\I_m$.
    \[
        \text{det}(A - \lambda\I_m) = \text{det}(A - \lambda(A^{-1}A)) =     \text{det}(A)\text{det}(\I_m - \lambda A^{-1}) = 0 
    \]
    \[
        \text{det}(I_m - \lambda A^{-1}) = -\text{det}(A^{-1} - \frac{1}{\lambda}\I_m) = 0
    \]
    \[
        \text{det}(A^{-1} - \frac{1}{\lambda}\I_m) = 0
    \]
    Therefore, $\lambda^{-1}$ is an eigenvalue of $A^{-1}$.
    
    \end{proof}


    \item 
    Let $A, B \in \C^{m\times m}$. Show that $AB$ and $BA$ have the same eigenvalues.
    
    \begin{proof}
    
    Let $A, B$ be square matrices as shown above. Now look at some eigenvalue of the   matrix product $AB$, $\lambda$. We have by definition of an eigenvalue the following equality.
    \[
        AB\vec{v} = \lambda\vec{v}
    \]
    Now we multiply both vectors by the matrix $B$.
    \[
        B(AB\vec{v}) = B(\lambda\vec{v})
    \]
    \[
       (BA)(B\vec{v}) = \lambda(B\vec{v})  
    \]
    \[
        BA\vec{w} = \lambda\vec{w}
    \]
    Therefore $\lambda$ is also an eigenvalue of the matrix product $BA$. Since our choice of $\lambda$ was arbitrary, we have that all eigenvalues of $AB$ are eigenvalues of $BA$. 
    \end{proof}


    \item 
    Let $A \in \R^{m \times m}$. Show that $A$ and $A^*$ have the same eigenvalues. (Hint 1: Use det($M$) = det($M^T$) for any square matrix $M \in \R^{m\times m}$ in connection to the definition of characteristic polynomials. Hint 2: When a real-valued matrix $A$ has a complex eigenvalue $\lambda$, then $\overline{\lambda}$ is also an eigenvalue of $A$.)

\begin{proof}
    
First look at an an arbitrary eigenvalue, $\lambda$, of $A$.
\[
    \text{det}(A - \lambda\I) = 0
\]
We look at two cases, 1. $\lambda \in \R$, and 2. $\lambda \in \C$. 

            \begin{enumerate}

                \item[Case 1:] 

                $\lambda \in \R$.
                We have since $A, \I, \lambda$ are all real-valued, that the conjugate transpose of $(A - \lambda\I)^*$ is equal to the transpose of the same matrix quantity. i.e.
                \[
                    (A - \lambda\I)^* = (A - \lambda\I)^T = A^T - \lambda\I = A^* - \lambda\I
                    \]
Therefore we can write, 
\[
    \text{det}(A - \lambda\I) = \text{det}(A^* - \lambda\I) = 0
\]
We can immediately see that any real eigenvalue of $A$ is also an eigenvalue of $A^*$. 

                \item[Case 2:] 
                    $\lambda \in \C$, We again look at the conjugate transpose,
                \[
                    (A - \lambda\I)^* = (A^* - \overline{\lambda}\I)
                \]
            
                
            \end{enumerate}
        
        \end{proof}
    \end{enumerate}

\item
Let $A \in \C^{m \times m}$ be hermitian. Suppose that for nonzero eigenvectors of $A$, there exist corresponding eigenvalues $\lambda$ satisfying $Ax = \lambda x$.

\begin{enumerate}
\item[a]
Prove that all eigenvalues of A are real.

\begin{proof}
    We look at an arbitary eigenvalue of $A$. 
\[
    Ax = \lambda x, x \in \C^m
\]
we multiple both sides by the conjugate transpose of $x$.
\[
    x^*(Ax) = x^*(\lambda x)
\]
\[  
    x^*Ax = \lambda(x^*x)
\]
We should notice that $x^*x$ is a scalar with a real value. This is because each component of $x$ is multiplied against its complex conjugate. Next we look at the dimensions and hermitian quantity of $x^*Ax$. We have that $x^* \in \C^{1 \times m}$, otherwise known as a row vector. We also have, $Ax \in \C^m$. Thereby, the matrix product of $x^*$ and $Ax$ is a $1\times 1$ quantity, a scalar! More importantly we have, 
\[
    (x^*Ax)^* = x^*A^*(x^*)^* = x^*Ax
\]
So, $x^*Ax$ is hermitian, or rather, $x^*Ax$ is a real-valued scalar. We then have, 
\[
    x^*Ax = \lambda(x^*x)
\]
Where both $x^*Ax$ and $x^*x$ are real valued, so consequently $\lambda \in \R$.
\end{proof}

\item[b.]
Let x and y be eigenvectors corresponding to distinct eigenvalues.
Show that (x, y) = 0, i.e., they are orthogonal. (Hint: Use the result
of Part (a).)

\begin{proof}

By the quaality that $A$ is hermition, we have for any two vectors, $x, y \in \C^m$, that
\[
    (Ax, y) = x^*A^*y = x^*Ay = (x, Ay)
\]
Therefore we can say for distinct eigenvectors, $v_1, v_2$ ($v_1 \neq v_2$), with distinct eigenvalues, $\lambda_1, \lambda_2$ ($\lambda_1 \neq \lambda_2$),   
\[
    (Av_1, v_2) - (v_1, Av_2) = 0
\]
\[
    = (\lambda_1v_1, v_2) - (v_1, \lambda_2v_2) = \overline{\lambda_1}v_1^*v_2 - v_1^*\lambda_1v_2
\]
\[
    = (\overline{\lambda_1} - \lambda_2)v_1^*v_2 = 0
\]
There are two things to notice, first since all eigenvalues are real, $\overline{\lambda_1} = \lambda_1$. Second, by our construction of the problem, $\lambda_1 \neq \lambda_2$. Thereby, $(\overline{\lambda_1} - \lambda_2) \neq 0$. So,
\[
    v_1^*v_2 = 0 = (v_1, v_2)
\]
\end{proof}
\end{enumerate} % end of 3

%Problem 4
\item
A matrix $A$ is called positive definite if and only if $(Ax, x) > 0$ for all $x \neq 0$
in $\C^m$. Suppose $A$ is Hermitian. Show that $A$ is positive definite if and
only if $\lambda_i > 0, \forall \lambda_i \in \Lambda(A)$, the spectrum of A.

\begin{proof}

By the property of $A$ being hermitian, that we can write any vector, $x \in \C_m, x \neq \vec{0}$ as the linear combination of the orthonormal eigenvectors of $A$, $u_i$. 
\[
    x = \alpha_1u_1 + \cdots + \alpha_mu_m
\]
We then look the inner product, $(Ax, x)$. 
\[
    Ax = A(\alpha_1u_1 + \cdots + \alpha_mu_m) = \lambda_1\alpha_1u_1 + \cdots + \lambda_m\alpha_mu_m
\]
\[
    (Ax)^* = \overline{\lambda_1\alpha_1} u_1^* + \cdots + \overline{\lambda_m\alpha_m}u_m^*
\]
\[
    (Ax, x) = (\overline{\lambda_1\alpha_1} u_1^* + \cdots + \overline{\lambda_m\alpha_m}u_m^*)(\alpha_1u_1 + \cdots + \alpha_mu_m)
\]
Here by the property of an orthonormal vector set, we have that $u_i^*u_j = 0$ if $i \neq j$ and $= 1$ if $i = j$.
\[
    (Ax, x) = \overline{\lambda_1\alpha_1}\alpha_1 + \cdots + \overline{\lambda_m\alpha_m}\alpha_m = \sum_{i=1}^m \lambda_i |\alpha_i|^2
\]
Of course, $|\alpha_i|^2$ is a strictly positive value. So for $(Ax, x) < 0$ we need at least one $\lambda_i < 0$. In fact, it is the case that if even one $\lambda_i < 0$ that $(Ax, x) \ngtr 0$ for all $x \in \C^m$. To prove that $(Ax, x) > 0, \forall x \in \C^m$, we take the case of only the smallest $\lambda_i$, $\lambda_k < 0$ (i.e. $|\lambda_k| < |\lambda_i| , \forall \lambda_i \in (\Lambda(A) - \{\lambda_k\})$). We can show by counter-example
\[
    \lambda_k < 0, x \in \C^m, x = \alpha_1u_1 + \cdots + \alpha_mu_m
\]
\[
    (Ax, x) = \lambda_k|\alpha_k|^2 + \sum_{i = 1, i\neq k}^m \lambda_i|\alpha_i|^2 
\]
\[
    \exists x_* \in \C^m, \text{ such that } |\alpha_k|^2 = \frac{1}{\lambda_k}\sum_{i = 1, i \neq k}^m \lambda_i|\alpha_i|^2 + 1
\]
\[
    (Ax_*, x_*) < 0, \text{ by construction.}
\]
\end{proof} %end of problem 4

\item Suppose A is unitary.

    \begin{enumerate}
    
    \item[(a)] Let $(\lambda, x)$ be an eigenvalue-vector pair of $A$. Show $\lambda$ satisfies $|\lambda| = 1$.
    \begin{proof}
        Since $A$ is unitary, we have that it preserves the angle and length of vectors under transformations. (i.e $(Ax, Ax) = (x, x)$ for any vector $x \in \C^m$). Thereby we have, 
\[
    (Ax, Ax) = (\lambda x, \lambda x) = \overline{\lambda}x^* \lambda x = |\lambda|^2 x^* x = |\lambda|^2 (x, x)
\]
\[
    (x, x) = (Ax, Ax) = |\lambda|^2(x, x) \implies |\lambda|^2 = 1
\]
\[
    |\lambda| = 1
\]
    \end{proof}

    \item[(b)] Prove or disprove $||A||_F = 1$

    \begin{proof}

        We have from the definition of the Frobenius Norm and since A is unitary, 

        \[
            ||A||_F = \sqrt{\tr{A^*A}} = \sqrt{\tr{\I}}
        \]
        Assume now that $\I \in \R^{m\times m}$. Then, $\tr{\I} = m$
        \[
            ||A||_F = \sqrt{m}
        \]
        Therefore, $||A||_F \neq 1$ unless, $A \in \C^{1\times 1}$ i.e. A is a scalar. In general though, for any $A \in \C^{m\times n}$ where $m, n > 1$, $||A||_F \neq 1$. 
    \end{proof}

    \end{enumerate}
    
    


\end{enumerate}



\end{document}
